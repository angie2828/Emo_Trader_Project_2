{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cdbbbdc-cc7d-4697-a5a1-2cd8c0b66cba",
   "metadata": {},
   "source": [
    "## Part 1 - Notebook Setup and Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dee4bd59-61f9-4cec-b1fc-5cfd1ba5d00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/fvaladrien/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "129a12453da84b7f8c40d758b9829d81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.2.2.json:   0%|   â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-24 19:42:06 INFO: Downloading default packages for language: en (English)...\n",
      "2021-09-24 19:42:06 INFO: File exists: /Users/fvaladrien/stanza_resources/en/default.zip.\n",
      "2021-09-24 19:42:09 INFO: Finished downloading models and saved to /Users/fvaladrien/stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "# Use model.fit(verbose=0) to avoid extra long notebook\n",
    "# To solve the error \"tweeperror: Failed to send request: Only unicode objects are escapable. Got None of type <class 'NoneType'>.v\" when extracting tweet Loaded Future\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "# General dependencies\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime, date, timedelta\n",
    "\n",
    "# For Twitter API extraction\n",
    "import tweepy\n",
    "#!pip install python-dotenv\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Tweet pre-processor\n",
    "import preprocessor as p\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# TextBlob\n",
    "from textblob import TextBlob\n",
    "\n",
    "# FLairNLP\n",
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "\n",
    "# Stanza\n",
    "import stanza\n",
    "stanza.download('en')\n",
    "\n",
    "# Stanford CoreNLP\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1ac284f-c1f4-4485-b3f9-7872085027e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter API credentials setup\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv('api_key')\n",
    "api_key_secret = os.getenv('api_key_secret')\n",
    "access_token = os.getenv('access_token')\n",
    "access_token_secret = os.getenv('access_token_secret')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb80b555-f725-4c5b-b8de-0ba6001b8bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(api_key, api_key_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afbbd8a-c0c4-4d99-9608-eefdb16beb1e",
   "metadata": {},
   "source": [
    "## Part 2 - Data Extraction with Twitter API and Text Pre-Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d92125f3-e415-48fa-a2e7-d321c43efe05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate list of dates (7 days window) based on today's date\n",
    "list_of_dates = []\n",
    "today = date.today()\n",
    "for i in range(-7,1):\n",
    "    target_date = (today + timedelta(days=i)).strftime(\"%Y-%m-%d\")\n",
    "    list_of_dates.append(target_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49159978-4ad8-47c0-9497-86926c85be8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_dicts = []\n",
    "search_term = 'covid19 covid vaccine'\n",
    "num_tweets = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ee5c3ef-2ca1-4b21-b106-a22d937b3c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets(search_term = search_term, num_tweets = num_tweets):\n",
    "    \n",
    "    for end_date in list_of_dates:\n",
    "        start_date = (datetime.strptime(end_date, '%Y-%m-%d') - timedelta(days=1)).strftime(\"%Y-%m-%d\") # Create 1-day windows for extraction\n",
    "        tweet_count = len(list_of_dicts)\n",
    "\n",
    "        for tweet in tweepy.Cursor(api.search,\n",
    "                                   q=f'{search_term} since:{start_date} until:{end_date}',\n",
    "                                   lang = 'en',\n",
    "                                   count = num_tweets,\n",
    "                                   tweet_mode = 'extended').items(num_tweets):\n",
    "            if (not tweet.retweeted) and ('RT @' not in tweet.full_text):\n",
    "                if tweet.lang == \"en\":\n",
    "                    tweet_dict = {}\n",
    "                    tweet_dict['username'] = tweet.user.name\n",
    "                    tweet_dict['location'] = tweet.user.location\n",
    "                    tweet_dict['text'] = tweet.full_text\n",
    "                    #tweet_dict['fav_count'] = tweet.favorite_count  \n",
    "                    tweet_dict['hashtags'] = tweet.entities['hashtags']\n",
    "                    tweet_dict['tweet_date'] = tweet.created_at\n",
    "                    list_of_dicts.append(tweet_dict)\n",
    "                    tweet_count +=1\n",
    "                    print(f'Extracted tweet count = {tweet_count}')\n",
    "                \n",
    "        print(f'Completed extraction for {start_date} to {end_date}. Sleep for 15 mins')\n",
    "        time.sleep(900)\n",
    "        print('Ready to go again')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b928244d-ce66-4ca9-b418-d0212769e120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run tweet extraction function\n",
    "get_tweets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b39700-de1b-4b45-a473-21af455abb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of tweets pulled\n",
    "len(list_of_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c748aa-148b-4292-89c4-072077f66f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform list of dictionaries into a Pandas dataframe\n",
    "tweets_df = pd.DataFrame(list_of_dicts)\n",
    "tweets_df.sort_values(by='tweet_date').reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb9866f-f519-4a1f-a49f-17e0269630f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup function to extract hashtags text from the raw hashtag dictionaries\n",
    "def extract_hashtags(hashtag_list):\n",
    "    \n",
    "    s = \"\" # Create empty string\n",
    "    if not hashtag_list: # If list is empty, return empty string\n",
    "        return s\n",
    "    else:\n",
    "        for dictionary in hashtag_list:\n",
    "            s+= str(dictionary['text'].lower() + ',') # Create string (lowercase) for each hashtag text\n",
    "        s = s[:-1] # Drop last character ','\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93520835-0628-4cc1-98b7-f43bb1b429a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract hashtags\n",
    "tweets_df['hashtags_extracted'] = tweets_df['hashtags'].apply(lambda x: extract_hashtags(x))\n",
    "tweets_df.drop(columns = 'hashtags', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c6f834-da32-4956-b5b2-18a2b2131e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd381d1-7f64-4f31-a9c8-e45377abfd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only tweets that involve the vaccine\n",
    "tweets_df_final = tweets_df[(tweets_df['text'].str.contains(\"vacc\")) \n",
    "                            | (tweets_df['text'].str.contains(\"Vacc\"))\n",
    "                            | (tweets_df['hashtags_extracted'].str.contains(\"vacc\"))\n",
    "                            | (tweets_df['hashtags_extracted'].str.contains(\"Vacc\"))]\n",
    "len(tweets_df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1750c505-75d3-4d4b-ba90-f054613fd690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create timestamp for datetime of extraction\n",
    "extract_datetime = datetime.today().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# Create csv filename\n",
    "filename = 'Data/covid_vaccine_tweets_extracted_' + extract_datetime + '.csv'\n",
    "\n",
    "# Drop duplicates (if any)\n",
    "tweets_df_final.drop_duplicates(inplace = True)\n",
    "\n",
    "# Export dataframe as csv file with above filename\n",
    "tweets_df_final.to_csv(filename, index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c630ff0b-b023-4da4-98de-5f94e0128b29",
   "metadata": {},
   "source": [
    "## Text Pre-processing\n",
    "Using tweet-preprocessor Python package (https://pypi.org/project/tweet-preprocessor/)\n",
    "pip install tweet-preprocessor\n",
    "\n",
    "Preprocessor is a preprocessing library for tweet data written in Python. Currently supports  # cleaning, tokenizing and parsing: URLs, Hashtags, Mentions, Reserved words (RT, FAV), Emojis, Smileys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a35d1f-055e-4d8c-b6f3-acbe41328fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all tweet text to lowercase\n",
    "# tweets_df['text'] = tweets_df['text'].apply(lambda x: str(x.lower()))\n",
    "\n",
    "# Note, skipping this step as uppercase reflects sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cffada-13ef-4e05-9e56-7a3c5a0f4a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = tweets_df_final.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156e844c-84cc-438f-8026-f8c8d09e880f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean tweet text with tweet-preprocessor\n",
    "tweets_df['text_cleaned'] = tweets_df['text'].apply(lambda x: p.clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd087448-c222-4416-a1de-820168e7e7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate tweets\n",
    "tweets_df.drop_duplicates(subset='text_cleaned', keep=\"first\", inplace = True)\n",
    "len(tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c633dbeb-63f1-4446-8017-b37af0aa2091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary characters\n",
    "# Note: Need to remove % as Stanford CoreNLP annotation encounters error if text contains some of these characters\n",
    "punct =['%','/',':','\\\\','&amp;','&',';']\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    for punctuation in punct:\n",
    "        text = text.replace(punctuation, '')\n",
    "    return text\n",
    "\n",
    "tweets_df['text_cleaned'] = tweets_df['text_cleaned'].apply(lambda x: remove_punctuations(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5c251b-6a53-45dc-a3b9-352fd843ea83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary characters\n",
    "# Note: Need to remove % as Stanford CoreNLP annotation encounters error if text contains some of these characters\n",
    "punct =['%','/',':','\\\\','&amp;','&',';']\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    for punctuation in punct:\n",
    "        text = text.replace(punctuation, '')\n",
    "    return text\n",
    "\n",
    "tweets_df['text_cleaned'] = tweets_df['text_cleaned'].apply(lambda x: remove_punctuations(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2018b81e-ff85-4299-a433-a2dda1263c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = tweets_df.reset_index(drop=True)\n",
    "tweets_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bec904f-3512-4674-8236-29183ad31219",
   "metadata": {},
   "source": [
    "## Part 3 - Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09d8f83-274d-401c-893c-04938457512f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to get value counts\n",
    "def get_value_counts(col_name, analyzer_name):\n",
    "    count = pd.DataFrame(tweets_df[col_name].value_counts())\n",
    "    percentage = pd.DataFrame(tweets_df[col_name].value_counts(normalize=True).mul(100))\n",
    "    value_counts_df = pd.concat([count, percentage], axis = 1)\n",
    "    value_counts_df = value_counts_df.reset_index()\n",
    "    value_counts_df.columns = ['sentiment', 'counts', 'percentage']\n",
    "    value_counts_df.sort_values('sentiment', inplace = True)\n",
    "    value_counts_df['percentage'] = value_counts_df['percentage'].apply(lambda x: round(x,2))\n",
    "    value_counts_df = value_counts_df.reset_index(drop = True)\n",
    "    value_counts_df['analyzer'] = analyzer_name\n",
    "    return value_counts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b909b671-d33d-4499-a9fc-40ba41df3b9f",
   "metadata": {},
   "source": [
    "## Part 3A - Sentiment Analysis with NLTK Vader\n",
    "Natural Learning Toolkit (NLTK) is a Python package that offers programs supporting natural language processing (NLP). In addition to its text corpus, it also comes with pre-trained models.  In particular, we will be using the Valence Aware Dictionary and sEntiment Reasoner (VADER) model, which is a lexicon and rule-based sentiment analysis tool specifically aimed at sentiment analysis of social media text. It uses a bag of words approach with simple heuristics (such as increasing sentiment intensity in presence of certain words like \"very\" or \"really\").\n",
    "\n",
    "After installing NLTK with the command pip install nltk, we can run sentiment analysis using VADER with these lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f75c91-aa88-4199-a792-1e02ab291095",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Obtaining NLTK scores\n",
    "tweets_df['nltk_scores'] = tweets_df['text_cleaned'].apply(lambda x: sia.polarity_scores(x))\n",
    "\n",
    "# Obtaining NLTK compound score\n",
    "tweets_df['nltk_cmp_score'] = tweets_df['nltk_scores'].apply(lambda score_dict: score_dict['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a4caad-ef84-4f6b-ba73-029f0cca2027",
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_thresh = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cda692-c7ea-48bf-940c-dcaff8434d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize scores into the sentiments of positive, neutral or negative\n",
    "tweets_df['nltk_sentiment'] = tweets_df['nltk_cmp_score'].apply(lambda c: 'Positive' if c >= neutral_thresh else ('Negative' if c <= -(neutral_thresh) else 'Neutral'))\n",
    "\n",
    "# Neutral score = 0\n",
    "# tweets_df['nltk_sentiment'] = tweets_df['nltk_cmp_score'].apply(lambda c: 'Positive' if c > 0 else ('Negative' if c < 0 else 'Neutral'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0955aa-220e-4424-9bdb-8257a3b2bd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['nltk_cmp_score'].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64ad44c-933e-422c-bec6-f97cb036473a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_sentiment_df = get_value_counts('nltk_sentiment','NLTK Vader')\n",
    "nltk_sentiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9054c9e8-3e1c-425e-85ee-dcb3a440e76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"dark\")\n",
    "ax = sns.barplot(x=\"sentiment\", y=\"percentage\", data=nltk_sentiment_df)\n",
    "ax.set_title('NLTK Vader')\n",
    "\n",
    "for index, row in nltk_sentiment_df.iterrows():\n",
    "    ax.text(row.name,row.percentage, round(row.percentage,1), color='black', ha=\"center\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24923abc-51e3-492c-83fa-b1a700c7c3aa",
   "metadata": {},
   "source": [
    "## Part 3B - Sentiment Analysis with TextBlob\n",
    "TextBlob is a popular Python library used to process textual data and perform a range of NLP tasks including sentiment analysis. Similar to NLTK Vader, the TextBlob sentiment classifier is also based on a bag of words approach. In fact, TextBlob is built upon the NLTK and pattern libraries.\n",
    "\n",
    "This is the command to install TextBlob: pip install textblob\n",
    "\n",
    "The NaiveBayesAnalyzer is trained on movies review dataset, so I will be using the default PatternAnalyzer instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2865bc11-13e5-41d9-ab24-5786dbfd9600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain polarity scores generated by TextBlob\n",
    "tweets_df['textblob_score'] = tweets_df['text_cleaned'].apply(lambda x: TextBlob(x).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc980087-8e0f-4b19-8371-fe1cd1bf1b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_thresh = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a215ae5-a84a-43a0-a676-c3a5928fbe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert polarity score into sentiment categories\n",
    "tweets_df['textblob_sentiment'] = tweets_df['textblob_score'].apply(lambda c: 'Positive' if c >= neutral_thresh else ('Negative' if c <= -(neutral_thresh) else 'Neutral'))\n",
    "\n",
    "# Neutral score = 0\n",
    "#tweets_df['textblob_sentiment'] = tweets_df['textblob_score'].apply(lambda c: 'Positive' if c > 0 else ('Negative' if c < 0 else 'Neutral'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409f5ae9-e69f-4836-86bb-f18a552a3dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['textblob_score'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53ce493-1ea8-4910-b23f-eb4bbaf3792c",
   "metadata": {},
   "outputs": [],
   "source": [
    "textblob_sentiment_df = get_value_counts('textblob_sentiment','TextBlob')\n",
    "textblob_sentiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a765dfcc-0520-477e-8595-3fce71ea5339",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"dark\")\n",
    "ax = sns.barplot(x=\"sentiment\", y=\"percentage\", data=textblob_sentiment_df)\n",
    "ax.set_title('TextBlob')\n",
    "\n",
    "for index, row in textblob_sentiment_df.iterrows():\n",
    "    ax.text(row.name,row.percentage, round(row.percentage,1), color='black', ha=\"center\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a87a16-5e38-475d-8489-b7f9ac31fae6",
   "metadata": {},
   "source": [
    "## Part 3C - Sentiment Analysis with Stanza\n",
    "Stanza is the default Python NLP library of the Stanford NLP Group, replacing the older Java-based CoreNLP. The modules are built on top of PyTorch, and its pre-built sentiment analyzer is trained on several datasets, including the Stanford Sentiment Treeback and Airline Twitter Sentiment.\n",
    "\n",
    "Score mapping:\n",
    "0: Negative\n",
    "1: Neutral\n",
    "2: Positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dfa05b-9406-4936-a7b3-04f75cc085a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f789e457-b273-47e7-93c0-4ec5ad8365e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stanza_analyze(Text):\n",
    "    document = nlp(Text)\n",
    "    print('Processing')\n",
    "    return np.mean([(i.sentiment - 1) for i in document.sentences]) # Minus 1 so as to bring score range of [0,2] to [-1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457c97aa-cfe9-443d-8ecd-ab0f8449711f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain sentiment categorical score generated by Stanza\n",
    "tweets_df['stanza_score'] = tweets_df['text_cleaned'].apply(lambda x: stanza_analyze(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ed8499-57d0-4382-98a7-dc893cbb173e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corenlp_senti_scores_df = pd.DataFrame(corenlp_senti_scores, columns = [\"corenlp_score\"])\n",
    "corenlp_senti_scores_df[\"corenlp_score\"] = pd.to_numeric(corenlp_senti_scores_df[\"corenlp_score\"])\n",
    "corenlp_senti_scores_df[\"corenlp_score\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70712206-4a0c-4164-88cc-3c28b1c7272b",
   "metadata": {},
   "source": [
    "## These CoreNLP scores correspond to the following:\n",
    "0: Very Negative\n",
    "1: Negative\n",
    "2: Neutral\n",
    "3: Positive\n",
    "4: Very Positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724118e2-6c71-40c9-91cd-ed6228a228b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate both dataframes\n",
    "tweets_df = pd.concat([tweets_df, corenlp_senti_scores_df], axis=1)\n",
    "\n",
    "# Map CoreNLP sentiments to scores\n",
    "corenlp_mapping = {0:'Negative',1:'Negative',2:'Neutral',3:'Positive',4:'Positive'}\n",
    "\n",
    "tweets_df['corenlp_sentiment'] = tweets_df['corenlp_score'].map(corenlp_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2f81b9-684a-47cd-b6e6-6e58d7dca8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "corenlp_sentiment_df = get_value_counts('corenlp_sentiment','CoreNLP')\n",
    "corenlp_sentiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5288600-1d25-4f06-86ab-382122fca8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"dark\")\n",
    "ax = sns.barplot(x=\"sentiment\", y=\"percentage\", data=corenlp_sentiment_df)\n",
    "ax.set_title('Stanford CoreNLP')\n",
    "\n",
    "for index, row in corenlp_sentiment_df.iterrows():\n",
    "    ax.text(row.name,row.percentage, round(row.percentage,1), color='black', ha=\"center\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cb5508-6a86-467f-8ab7-3273626e4cb2",
   "metadata": {},
   "source": [
    "## Part 4 - Insights from Sentiment Analyses\n",
    "We will focus on the results from NLTK VADER, TextBlob and Stanza because they are:\n",
    "\n",
    "Trained on at least 1 social media dataset\n",
    "Able to give at least 3 classes of sentiments i.e. Positive, Neutral, Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b590bc1-0e5f-4da1-809f-3dc1bb027ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiments = pd.concat([nltk_sentiment_df, \n",
    "                           textblob_sentiment_df, \n",
    "                           stanza_sentiment_df,\n",
    "                           #flair_sentiment_df,\n",
    "                           #corenlp_sentiment_df,\n",
    "                          ]).reset_index(drop=True)\n",
    "df_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969e3773-95be-4528-b848-5282b3f15c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiments_pivot = df_sentiments.pivot(index='sentiment', columns='analyzer', values='percentage')\n",
    "df_sentiments_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960c5fba-a2a4-432e-a2a9-a9d9c83bf062",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "ax = sns.barplot(x=\"analyzer\", y=\"percentage\",\n",
    "                 hue=\"sentiment\", data=df_sentiments)\n",
    "\n",
    "# Display annotations\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f\"{round(p.get_height(),1)}%\", \n",
    "                   (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                   ha = 'center', va = 'center', \n",
    "                   size=12,\n",
    "                   xytext = (0, -12), \n",
    "                   textcoords = 'offset points')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade12bf3-868a-4104-8a70-665ee3081d3c",
   "metadata": {},
   "source": [
    "## Part 5 - Composite Sentiment with Ensemble Method\n",
    "Average Score\n",
    "Take average of the 3 sentiment scores of NLTK Vader, TextBlob and Stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452fb0de-0ac2-4375-9612-36486260c02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make use of sentiments from NLTK Vader, TextBlob and Stanza\n",
    "tweets_df['composite_score'] =  (tweets_df['nltk_cmp_score'] \n",
    "                                + tweets_df['textblob_score']\n",
    "                                + tweets_df['stanza_score'])/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f992520-ddb3-44c6-a592-191672cf6c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['composite_score'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae86f5d6-b029-45dc-bca5-193e0ebe458f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold for neutral sentiment\n",
    "neutral_thresh = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ddd8fb-9612-4006-8952-fa47e8c09b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert average sentiment score (from all 3 analyzers) into sentiment categories\n",
    "tweets_df['composite_vote_2'] = tweets_df['composite_score'].apply(lambda c: 'Positive' if c >= neutral_thresh else ('Negative' if c <= -(neutral_thresh) else 'Neutral'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1c4e5b-14b6-4405-9af9-3dea77716be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "composite_sentiment_df_2 = get_value_counts('composite_vote_2','Composite Sentiment')\n",
    "composite_sentiment_df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdf0ac1-5fb2-4ab1-b953-b535f114e1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "ax = sns.barplot(x=\"sentiment\", y=\"percentage\",\n",
    "                 data=composite_sentiment_df_2)\n",
    "\n",
    "# Display annotations\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f\"{round(p.get_height(),1)}%\", \n",
    "                   (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                   ha = 'center', va = 'center', \n",
    "                   size=12,\n",
    "                   xytext = (0, -12), \n",
    "                   textcoords = 'offset points')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4706c1cb-71aa-48c4-a382-f857182060dd",
   "metadata": {},
   "source": [
    "## Experiment: Max Voting\n",
    "Get composite sentiment by doing max voting amongst the 3 analyzers NLTK Vader, TextBlob and Stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143a81fc-c361-44f6-8a79-88bef8e97ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make use of sentiments from NLTK Vader, TextBlob and Stanza\n",
    "tweets_df['sentiment_votes'] =  tweets_df.apply(lambda x: list([x['nltk_sentiment'], \n",
    "                                                                x['textblob_sentiment'], \n",
    "                                                                x['stanza_sentiment']]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e0464c-4c94-496e-b40d-84f57cf6568b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to get sentiment that appears most often amongst the 3 votes\n",
    "def get_most_voted_senti(List):\n",
    "    if len(List) == len(set(List)): # If all elements are different\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return max(set(List), key = List.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5272d2b-da81-4fb6-aaa8-03fa0a2c348f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get composite sentiment vote\n",
    "tweets_df['composite_vote'] = tweets_df['sentiment_votes'].apply(lambda x: get_most_voted_senti(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1414a3d-fb45-4d5d-831e-1e09ac0d00d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "composite_sentiment_df = get_value_counts('composite_vote','Composite Sentiment (Max Voting)')\n",
    "composite_sentiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8248a8a8-f45f-45d8-a787-adf3a5e981b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "ax = sns.barplot(x=\"analyzer\", y=\"percentage\",\n",
    "                 hue=\"sentiment\", data=composite_sentiment_df)\n",
    "\n",
    "# Display annotations\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f\"{round(p.get_height(),1)}%\", \n",
    "                   (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                   ha = 'center', va = 'center', \n",
    "                   size=12,\n",
    "                   xytext = (0, -12), \n",
    "                   textcoords = 'offset points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451f7e67-bddf-4679-8229-03f77fc46d39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyviz0724]",
   "language": "python",
   "name": "conda-env-pyviz0724-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
